The framework integrates information from all channels of a 3D video, including RGB, depth, skeleton, and context objects, utilizing time, channel, and context fusion mechanisms. Each type of information undergoes analysis through dedicated 2D convolutional neural networks (CNNs), implemented using Google's AutoML tool. The final output is obtained by combining the outputs of all these specialized networks. The achieved high accuracies with the Motion History Image-based Convolutional Neural Networks (MHI-CNNs) validate the efficiency of compressing a video into an image using a pixel-fusion mechanism. Furthermore, the study demonstrates the effectiveness of multiple predictions, affirming that training several CNNs individually and combining their results leads to significantly improved classification accuracy. This approach contributes to the optimization of human activity recognition systems by capitalizing on fusion mechanisms, automated machine learning, and the advantageous properties of pixel-level fusion in video-to-image compression. Based on our conducted experiments, the presented framework has demonstrated attributes of being expeditiously trainable, owing to the incorporation of AutoML, and exhibits a high level of accuracy. Its highly customizable nature is evidenced by the ease with which modules, about temporal, channel, or context fusion, can be added or removed and subsequently trained. Furthermore, the introduction of a new RGB-D dataset in this study is expected to contribute significantly to the computer vision community, fostering the development of novel methods for human activity recognition. 
